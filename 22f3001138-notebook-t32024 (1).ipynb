{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":85062,"databundleVersionId":9578279,"sourceType":"competition"}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-30T15:44:13.226100Z","iopub.execute_input":"2024-11-30T15:44:13.226487Z","iopub.status.idle":"2024-11-30T15:44:13.234991Z","shell.execute_reply.started":"2024-11-30T15:44:13.226454Z","shell.execute_reply":"2024-11-30T15:44:13.233744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sklearn\nimport pandas\nimport numpy as np\nimport sklearn\nfrom sklearn.dummy import DummyClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder,OrdinalEncoder,OneHotEncoder,StandardScaler,MinMaxScaler\nfrom sklearn.model_selection import train_test_split,GridSearchCV, RandomizedSearchCV, cross_val_predict,cross_val_score,cross_validate\nfrom sklearn.feature_selection import SelectKBest,chi2,RFE,RFECV\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T15:44:14.233196Z","iopub.execute_input":"2024-11-30T15:44:14.233631Z","iopub.status.idle":"2024-11-30T15:44:14.240695Z","shell.execute_reply.started":"2024-11-30T15:44:14.233595Z","shell.execute_reply":"2024-11-30T15:44:14.239294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T15:44:14.553397Z","iopub.execute_input":"2024-11-30T15:44:14.553831Z","iopub.status.idle":"2024-11-30T15:44:14.559316Z","shell.execute_reply.started":"2024-11-30T15:44:14.553794Z","shell.execute_reply":"2024-11-30T15:44:14.557998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ntrain_data=pd.read_csv(\"/kaggle/input/predict-the-success-of-bank-telemarketing/train.csv\")\ntest_data=pd.read_csv(\"/kaggle/input/predict-the-success-of-bank-telemarketing/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T15:44:14.859984Z","iopub.execute_input":"2024-11-30T15:44:14.860344Z","iopub.status.idle":"2024-11-30T15:44:14.960781Z","shell.execute_reply.started":"2024-11-30T15:44:14.860312Z","shell.execute_reply":"2024-11-30T15:44:14.959601Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"markdown","source":"Input variables:  \n1 last contact date: last contact date  \n2 age (numeric)  \n3 job : type of job  \n4 marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" means divorced or widowed)  \n5 education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")  \n6 default: has credit in default? (binary: \"yes\",\"no\")  \n7 balance: average yearly balance, in euros (numeric)  \n8 housing: has housing loan? (binary: \"yes\",\"no\")  \n9 loan: has personal loan? (binary: \"yes\",\"no\")  \n10 contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\")  \n11 duration: last contact duration, in seconds (numeric)  \n12 campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)  \n13 pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)  \n14 previous: number of contacts performed before this campaign and for this client (numeric)  \n15 poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")  \n\nOutput variable (desired target):  \n16 target: has the client subscribed a term deposit? (binary: \"yes\",\"no\")  ","metadata":{}},{"cell_type":"code","source":"train_eda=train_data.copy() # perfrom eda on the copied dataset","metadata":{"execution":{"iopub.status.busy":"2024-11-23T05:03:58.336149Z","iopub.execute_input":"2024-11-23T05:03:58.336641Z","iopub.status.idle":"2024-11-23T05:03:58.345326Z","shell.execute_reply.started":"2024-11-23T05:03:58.336600Z","shell.execute_reply":"2024-11-23T05:03:58.344067Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#read the dataset\nprint(pd.DataFrame(train_eda.info()))\nprint(pd.DataFrame(train_eda.describe()))","metadata":{"execution":{"iopub.status.busy":"2024-11-10T08:05:11.610297Z","iopub.execute_input":"2024-11-10T08:05:11.610798Z","iopub.status.idle":"2024-11-10T08:05:11.708473Z","shell.execute_reply.started":"2024-11-10T08:05:11.610741Z","shell.execute_reply":"2024-11-10T08:05:11.706227Z"},"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_columns=train_eda.select_dtypes('object').columns\nnumerical_columns=train_eda.select_dtypes('int').columns\nprint('categorical features:',categorical_columns, 'numerical features:',numerical_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T08:05:14.367792Z","iopub.execute_input":"2024-11-10T08:05:14.368524Z","iopub.status.idle":"2024-11-10T08:05:14.388669Z","shell.execute_reply.started":"2024-11-10T08:05:14.368452Z","shell.execute_reply":"2024-11-10T08:05:14.386495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Is the data imbalanced?\n#check the target\ntarget_distri=train_eda['target'].value_counts()\nax=target_distri.plot.bar()\nplt.title(\"Target distribution\")\n\ntotal_count=target_distri.sum()\nfor p in ax.patches:\n    height=p.get_height()\n    percentage=(height/total_count) * 100\n    ax.annotate(f'{percentage:.1f}%', (p.get_x() + p.get_width() / 2, height), \n                ha='center', va='bottom')\n\nplt.xlabel('Target Classes')\nplt.ylabel('Count')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T08:05:15.816757Z","iopub.execute_input":"2024-11-10T08:05:15.817236Z","iopub.status.idle":"2024-11-10T08:05:16.175468Z","shell.execute_reply.started":"2024-11-10T08:05:15.817194Z","shell.execute_reply":"2024-11-10T08:05:16.173906Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"color:red\">The data is highly imbalanced :( </span>","metadata":{}},{"cell_type":"markdown","source":"#### Missing data distribution","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nmissing_distri = (train_eda.isna().sum() / train_eda.shape[0]).sort_values(ascending=False)\n\nax = missing_distri.plot.bar()\nplt.title('Missing Data Distribution')\n\ntotal_count = missing_distri.sum()\n\nfor p in ax.patches:\n    height = p.get_height()\n    percentage = (height / total_count) * 100\n    ax.annotate(f'{percentage:.1f}%', (p.get_x() + p.get_width() / 2, height), \n                ha='center', va='bottom')\n\n# Set labels and show the plot\nplt.xlabel('Features')\nplt.ylabel('Proportion of Missing Data')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T08:05:18.191268Z","iopub.execute_input":"2024-11-10T08:05:18.191850Z","iopub.status.idle":"2024-11-10T08:05:18.654521Z","shell.execute_reply.started":"2024-11-10T08:05:18.191795Z","shell.execute_reply":"2024-11-10T08:05:18.653021Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### univariate analysis","metadata":{}},{"cell_type":"markdown","source":"#### Categorical analysis","metadata":{"execution":{"iopub.status.busy":"2024-11-08T06:45:15.664883Z","iopub.execute_input":"2024-11-08T06:45:15.665350Z","iopub.status.idle":"2024-11-08T06:45:15.671587Z","shell.execute_reply.started":"2024-11-08T06:45:15.665309Z","shell.execute_reply":"2024-11-08T06:45:15.669908Z"}}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef feature_countplot(ax, x):\n    sns.countplot(data=train_eda, x=x, hue=\"target\", ax=ax)\n    for p in ax.patches:\n        count = p.get_height()\n        ax.annotate(f'{count:.2f}', (p.get_x() + p.get_width() / 2., count), \n                    ha='center', va='bottom', fontsize=12, color='black', \n                    rotation=0)\n\n\n\nncols = 2\nnrows = (len(categorical_columns) + 1) // ncols\n\n\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 5 * nrows))\naxes = axes.flatten()\n\nfor i, feature in enumerate(categorical_columns):\n    feature_countplot(axes[i], feature)\n    axes[i].set_title(f'Count plot for {feature}', fontsize=16)\n\nif len(categorical_columns) % ncols != 0:\n    fig.delaxes(axes[-1])\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T08:05:46.916716Z","iopub.execute_input":"2024-11-10T08:05:46.918048Z","iopub.status.idle":"2024-11-10T08:06:14.712237Z","shell.execute_reply.started":"2024-11-10T08:05:46.917971Z","shell.execute_reply":"2024-11-10T08:06:14.710887Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Categorical Insights**  \n<div style=\"font-family:Verdana, sans-serif; color:blue;\"> \n<li><u>job</u>: 11 categories , <b>blue-collar</b> and <b>management job</b> -  highest rate of negative responses </li>  \nmanagement - highest rate of positive responses\n<li><u>marital</u> - 3 categories, 58% married, out of them, 90% negative response</li>\n<li><u>education</u>- 3 categories, majority of them have secondary education and negative responses</li>\nNote: education and marital have a somewhat <i>similar</i> chart  \n<li><u>default</u>- around 95 percent of the clients do not have a credit default and out of these around 90 percent do not subscribe to the term deposit</li>\n<li><u>housing</u>- Slighlty even distribution b/w yes and no, although, in both yes and no, majority are not subscribed</li>\n<li><u>loan</u>- 80 percent dont have a loan, majority on both sides, not subscribed</li>\n<li><u>contact</u>- 2 categories , 25 percent values missing, 90 percent cellular</li>\n<li><u> poutcome</u>- 3 categories, 75 percent values missing, majority failure</li>\n</div>","metadata":{}},{"cell_type":"markdown","source":"#### bivariate categorical analysis","metadata":{}},{"cell_type":"code","source":"pd.crosstab(train_eda['marital'],train_eda['education'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T08:06:14.714912Z","iopub.execute_input":"2024-11-10T08:06:14.715345Z","iopub.status.idle":"2024-11-10T08:06:14.750077Z","shell.execute_reply.started":"2024-11-10T08:06:14.715301Z","shell.execute_reply":"2024-11-10T08:06:14.748573Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Numerical Analysis","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.tree import DecisionTreeRegressor\nimport numpy as np\nimport pandas as pd\n\ndef kmeans_binning(df, column_name, n_bins=4):\n    \"\"\"\n    K-means binning method.\n    \"\"\"\n    balance_values = df[column_name].dropna().values.reshape(-1, 1)\n    kmeans = KMeans(n_clusters=n_bins, random_state=0).fit(balance_values)\n    sorted_centroids = sorted(kmeans.cluster_centers_.flatten())\n    bin_edges = [df[column_name].min()] + sorted_centroids + [df[column_name].max()]\n    \n    return bin_edges\n\n\ndef decision_tree_binning(df, column_name, n_bins=4):\n    \"\"\"\n    Decision Tree-based binning method.\n    \"\"\"\n    balance_values = df[column_name].dropna().values.reshape(-1, 1)\n    decision_tree = DecisionTreeRegressor(max_leaf_nodes=n_bins, random_state=0)\n    decision_tree.fit(balance_values, np.zeros_like(balance_values))  # Fit a dummy target\n    bin_edges = np.sort(decision_tree.apply(balance_values).flatten())\n    \n    # Convert leaf node values to actual bin edges\n    bin_edges = np.unique(np.concatenate(([df[column_name].min()], bin_edges, [df[column_name].max()])))\n    \n    return bin_edges\n\ndef analyze_for_binning(df, column_name):\n    \"\"\"\n    Analyze the column for binning with K-means and Decision Tree-based binning.\n    \"\"\"\n    stats_dict = {\n        'count': len(df[column_name]),\n        'missing': df[column_name].isnull().sum(),\n        'min': df[column_name].min(),\n        'max': df[column_name].max(),\n        'mean': df[column_name].mean(),\n        'median': df[column_name].median(),\n        'std': df[column_name].std(),\n        'skew': df[column_name].skew()\n    }\n    \n    percentiles = [0, 1, 5, 10, 25, 50, 75, 90, 95, 99, 100]\n    for p in percentiles:\n        stats_dict[f'percentile_{p}'] = np.percentile(df[column_name], p)\n    \n    # Calculate frequency of unique values\n    value_counts = df[column_name].value_counts()\n    stats_dict['unique_values'] = len(value_counts)\n    stats_dict['top_5_common_values'] = value_counts.head().to_dict()\n    \n    # Method 1: Quartile-based bins\n    q1, q2, q3 = np.percentile(df[column_name], [25, 50, 75])\n    stats_dict['quartile_based_bins'] = {\n        'boundaries': [df[column_name].min(), q1, q2, q3, df[column_name].max()]\n    }\n    \n    # Method 2: Equal-width bins (Sturges' formula)\n    n_bins = int(np.ceil(np.log2(len(df))) + 1)\n    bin_width = (df[column_name].max() - df[column_name].min()) / n_bins\n    equal_width_bins = [df[column_name].min() + i * bin_width for i in range(n_bins + 1)]\n    stats_dict['equal_width_bins'] = {\n        'n_bins': n_bins,\n        'bin_width': bin_width,\n        'boundaries': equal_width_bins\n    }\n    \n    # Method 3: Logarithmic bins (if skewed)\n    if stats_dict['skew'] > 1:\n        log_bounds = np.exp(np.linspace(np.log(df[column_name].min() + 1), np.log(df[column_name].max()), 5))\n        stats_dict['distribution_based_bins'] = {'boundaries': log_bounds.tolist(), 'type': 'logarithmic'}\n    else:\n        mean = df[column_name].mean()\n        std = df[column_name].std()\n        stats_dict['distribution_based_bins'] = {\n            'boundaries': [df[column_name].min(), mean - std, mean, mean + std, df[column_name].max()],\n            'type': 'linear'\n        }\n    \n    # K-means Binning\n    stats_dict['kmeans_bins'] = {\n        'boundaries': kmeans_binning(df, column_name, n_bins=4)\n    }\n    \n    # Decision Tree Binning\n    stats_dict['decision_tree_bins'] = {\n        'boundaries': decision_tree_binning(df, column_name, n_bins=4)\n    }\n    \n    return stats_dict\n\ndef suggest_bins(df, column_name):\n    \"\"\"\n    Print human-readable suggestions for binning based on the analysis.\n    \"\"\"\n    analysis = analyze_for_binning(df, column_name)\n    \n    print(f\"\\nAnalysis for {column_name}:\")\n    print(\"-\" * 50)\n    \n    # Basic statistics\n    print(f\"Total records: {analysis['count']:,}\")\n    print(f\"Range: {analysis['min']:,.2f} to {analysis['max']:,.2f}\")\n    print(f\"Mean: {analysis['mean']:,.2f}\")\n    print(f\"Median: {analysis['median']:,.2f}\")\n    print(f\"Standard deviation: {analysis['std']:,.2f}\")\n    print(f\"Skewness: {analysis['skew']:.2f}\")\n    \n    print(\"\\nPercentile Distribution:\")\n    for p in [0, 10, 25, 50, 75, 90, 100]:\n        print(f\"{p}th percentile: {analysis[f'percentile_{p}']:,.2f}\")\n    \n    print(\"\\nMost Common Values:\")\n    for val, count in analysis['top_5_common_values'].items():\n        print(f\"Value {val:,.2f}: {count:,} occurrences\")\n    \n    print(\"\\nSuggested Binning Approaches:\")\n    \n    print(\"\\n1. Quartile-based bins:\")\n    boundaries = analysis['quartile_based_bins']['boundaries']\n    for i in range(len(boundaries)-1):\n        print(f\"Bin {i+1}: {boundaries[i]:,.2f} to {boundaries[i+1]:,.2f}\")\n    \n    print(\"\\n2. Equal-width bins:\")\n    boundaries = analysis['equal_width_bins']['boundaries']\n    for i in range(len(boundaries)-1):\n        print(f\"Bin {i+1}: {boundaries[i]:,.2f} to {boundaries[i+1]:,.2f}\")\n    \n    print(\"\\n3. Distribution-based bins:\")\n    boundaries = analysis['distribution_based_bins']['boundaries']\n    bin_type = analysis['distribution_based_bins']['type']\n    print(f\"Type: {bin_type}\")\n    for i in range(len(boundaries)-1):\n        print(f\"Bin {i+1}: {boundaries[i]:,.2f} to {boundaries[i+1]:,.2f}\")\n    \n    print(\"\\n4. K-means Binning:\")\n    boundaries = analysis['kmeans_bins']['boundaries']\n    for i in range(len(boundaries)-1):\n        print(f\"Bin {i+1}: {boundaries[i]:,.2f} to {boundaries[i+1]:,.2f}\")\n    \n    print(\"\\n5. Decision Tree Binning:\")\n    boundaries = analysis['decision_tree_bins']['boundaries']\n    for i in range(len(boundaries)-1):\n        print(f\"Bin {i+1}: {boundaries[i]:,.2f} to {boundaries[i+1]:,.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T05:03:47.222037Z","iopub.execute_input":"2024-11-23T05:03:47.222487Z","iopub.status.idle":"2024-11-23T05:03:47.373357Z","shell.execute_reply.started":"2024-11-23T05:03:47.222450Z","shell.execute_reply":"2024-11-23T05:03:47.371989Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nncols = 3 \nnrows = (len(train_eda.select_dtypes('int').columns) + ncols - 1) // ncols \n\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 5 * nrows))\naxes = axes.flatten()  \n\n\nfor i, col in enumerate(train_eda.select_dtypes('int').columns):\n    sns.histplot(train_eda[col], kde=True, ax=axes[i])  \n    axes[i].set_title(f'Distribution of {col}')\n\nfor j in range(i + 1, len(axes)):\n    axes[j].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T08:06:14.796050Z","iopub.execute_input":"2024-11-10T08:06:14.796642Z","iopub.status.idle":"2024-11-10T08:06:21.363766Z","shell.execute_reply.started":"2024-11-10T08:06:14.796573Z","shell.execute_reply":"2024-11-10T08:06:21.362060Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color:red\">Features such as balance, duration, campaign, pdays, previous are <i>highly skewed</i></div>","metadata":{}},{"cell_type":"markdown","source":"#### Analysis of the above variables and binning ideas\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T04:08:24.595100Z","iopub.execute_input":"2024-11-09T04:08:24.595629Z","iopub.status.idle":"2024-11-09T04:08:24.601238Z","shell.execute_reply.started":"2024-11-09T04:08:24.595581Z","shell.execute_reply":"2024-11-09T04:08:24.599920Z"}}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-family:Verdana, sans-serif; color:purple;font-size:16px\"> 📌 it's well-known that a tree(/forest) algorithm (xgboost/rpart/etc.) will generally 'prefer' continuous variables over binary categorical ones in its variable selection, since it can choose the continuous split-point wherever it wants to maximize the information gain </div>  \n\n<href>More on this: https://stackoverflow.com/questions/51601122/xgboost-minimize-influence-of-continuous-linear-features-as-opposed-to-categori</href>\n<p><href>https://stats.stackexchange.com/questions/171192/why-do-decision-trees-rpart-prefer-to-choose-continuous-over-categorical-variabl</href></p>","metadata":{"execution":{"iopub.status.busy":"2024-11-09T04:08:42.174271Z","iopub.execute_input":"2024-11-09T04:08:42.174714Z","iopub.status.idle":"2024-11-09T04:08:42.183486Z","shell.execute_reply.started":"2024-11-09T04:08:42.174674Z","shell.execute_reply":"2024-11-09T04:08:42.181758Z"}}},{"cell_type":"code","source":"#Analyzing balance\nfig,axes=plt.subplots(1,3,figsize=(14,6))\n\nsns.violinplot(x='target', y='balance', data=train_eda,ax=axes[0])\naxes[0].set_title('Violing Plot: Balance vs Target')\n\nfilt=train_eda[(train_eda['balance']>86641.58)]\nsns.countplot(x=\"target\",data= filt, ax=axes[1])\n# axis[1].set_title()\n\nfilt2=train_eda[(train_eda['balance']>86641)&(train_eda['balance']<100000)]\nsns.countplot(x=\"target\",data= filt2, ax=axes[2])\n#from 70,no begins to increase once again\n\nprint(f\"Number of entries <0, i.e negative balance is {(train_eda['balance']<0).sum()}\")\nprint(f\"Number of entries =0, i.e negative balance is {(train_eda['balance']==0).sum()}\")\n# _, bins = pd.qcut(train_eda['balance'], q=4, labels=['low', 'medium', 'high', 'very_high'], retbins=True)\n# print(\"Bin edges:\", [f\"{edge:.2f}\" for edge in bins])\nsuggest_bins(train_eda,\"balance\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T08:06:21.365450Z","iopub.execute_input":"2024-11-10T08:06:21.365877Z","iopub.status.idle":"2024-11-10T08:06:22.584387Z","shell.execute_reply.started":"2024-11-10T08:06:21.365834Z","shell.execute_reply":"2024-11-10T08:06:22.582944Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Insights from balance:\n<li>Need to take care of values <0!!</li>\n<li>@ >29021, probability of yes beings to level and increase, until probably 70</li>\n<li><b>K-Means binning approach seems to be more appropriate (if need be)</b></li>\n<li>balance >29021 and <70000, p(yes) ≥ p(no)</li>","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 6))\nsns.violinplot(x='target', y='duration', data=train_eda, ax=axes[0])\naxes[0].set_title('Violin Plot: Duration vs Target')\n\nover_65 = train_eda[(train_eda['duration'] > 650)]\n\nsns.countplot(x='target', data=over_65, ax=axes[1])\naxes[1].set_title('Subscription Distribution for Duration > 650')\n\nless_35=train_eda[(train_eda['duration'] < 200)]\nsns.countplot(x='target', data=less_35, ax=axes[2])\naxes[2].set_title('Subscription Distribution for Duration < 200')\n\nsuggest_bins(train_eda,\"duration\")\n# Show the plots\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T05:03:53.336155Z","iopub.execute_input":"2024-11-23T05:03:53.336529Z","iopub.status.idle":"2024-11-23T05:03:53.858193Z","shell.execute_reply.started":"2024-11-23T05:03:53.336499Z","shell.execute_reply":"2024-11-23T05:03:53.856072Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Very Interesting observation:**\n<div style=\"font-family:Verdana, sans-serif; color:green;\"> \n<p>Our dataset is heavily imbalanced, but, when duration is over 650, the distribution of people subsribing as opposed to Not, increases!! (well almost 50 50) 😃😃 </p> \n<p>and whenever the duration is less than 200 secs, very very high probability of no subscription</p>\n</div>","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nsns.violinplot(x='target', y='campaign', data=train_eda, ax=axes[0])\naxes[0].set_title('Violin Plot: Camapign vs Target')\n\nover_30 = train_eda[(train_eda['campaign'] > 30)]\n\nsns.countplot(x='target', data=over_30, ax=axes[1])\naxes[1].set_title('Subscription Distribution for Campaign > 30')\n\nsuggest_bins(train_eda,\"campaign\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T08:06:23.831458Z","iopub.execute_input":"2024-11-10T08:06:23.832136Z","iopub.status.idle":"2024-11-10T08:06:24.747123Z","shell.execute_reply.started":"2024-11-10T08:06:23.832078Z","shell.execute_reply":"2024-11-10T08:06:24.745658Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Insight**\n<div style=\"font-family:Verdana, sans-serif; color:green;\"> \n<p>and whenever the duration is >30 secs, both yes and no have similar probabilities</p>\n</div>","metadata":{}},{"cell_type":"code","source":"suggest_bins(train_eda,\"age\")\nfig,axes=plt.subplots(1,2,figsize=(14,6))\n\nsns.violinplot(x=\"target\",y=\"age\",data=train_eda,ax=axes[0])\naxes[0].set_title('Violin Plot: age vs target')\n\nfilt=train_eda[train_eda['age']>79]\n\nsns.countplot(x=\"target\",data=filt,ax=axes[1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T08:06:24.748884Z","iopub.execute_input":"2024-11-10T08:06:24.749277Z","iopub.status.idle":"2024-11-10T08:06:25.809307Z","shell.execute_reply.started":"2024-11-10T08:06:24.749236Z","shell.execute_reply":"2024-11-10T08:06:25.807975Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Insights age**:  \n- 79 is a key figure, p(yes) starts increasing\n","metadata":{}},{"cell_type":"code","source":"suggest_bins(train_eda,\"pdays\")\nfig,axes=plt.subplots(1,3,figsize=(14,6))\n\nsns.violinplot(x=\"target\",y=\"pdays\",data=train_eda,ax=axes[0])\naxes[0].set_title('Violin Plot: pdays vs target')\n\n\nsns.violinplot(x=\"target\",y=\"previous\",data=train_eda,ax=axes[1])\naxes[0].set_title('Violin Plot: previous vs target')\n\nsuggest_bins(train_eda,\"previous\")\n\n\nfilt=train_eda[train_eda['previous']>32]\n\nsns.countplot(x=\"target\",data=filt,ax=axes[2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T05:04:07.661997Z","iopub.execute_input":"2024-11-23T05:04:07.662422Z","iopub.status.idle":"2024-11-23T05:04:09.340434Z","shell.execute_reply.started":"2024-11-23T05:04:07.662386Z","shell.execute_reply":"2024-11-23T05:04:09.338876Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- The distribution of pdays and previous is <i>extremely similar</i>\n- Whenever pdays=-1 <-> previous=0\n- 75 percent of values in pdays are -1 and 75 percent values is previous are 0\n- 32 is when p(yes) > p(no) for previous","metadata":{}},{"cell_type":"markdown","source":"Some <u>very *interesting* insight</u>, whenever pdays is -1 <-> previous is 0 i.e   \n<span style=\"font-family:Verdana, sans-serif; color:blue;\"> if the candidate is not previously contacted (-1), the number of contacts performed before this campaign for this client is 0 </span>","metadata":{}},{"cell_type":"code","source":"sns.pairplot(train_eda)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T08:06:27.359873Z","iopub.execute_input":"2024-11-10T08:06:27.360253Z","iopub.status.idle":"2024-11-10T08:06:50.406194Z","shell.execute_reply.started":"2024-11-10T08:06:27.360214Z","shell.execute_reply":"2024-11-10T08:06:50.396964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ncorr_matrix = train_eda.select_dtypes('int').corr()\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n\nplt.title('Correlation Heatmap', fontsize=16)\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T08:06:50.408663Z","iopub.execute_input":"2024-11-10T08:06:50.409781Z","iopub.status.idle":"2024-11-10T08:06:51.001757Z","shell.execute_reply.started":"2024-11-10T08:06:50.409708Z","shell.execute_reply":"2024-11-10T08:06:51.000649Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Insights:**\n<div style=\"font-family:Verdana, sans-serif; color:blue;\"> \n<li>balance-previous highest corr 0.72   </li>\n<li> balance-duration, balance-campaign, duration-campaign, duration-previous, campaign-previous (60-70) </li>\n<li> balance-pdays, duration-pdays, campaign-pdays, previous-pdays(50-60)</li>\n<li> balance-campaign-duration-pdays-previous (0.50 to 0.70)</li>\n</div>","metadata":{}},{"cell_type":"markdown","source":"We are aware that our numerical variables are highly skewed, except for maybe age..so, ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Log transformation\ntrain_eda['campaign_log'] = np.log1p(train_eda['campaign'])\ntrain_eda['duration_log'] = np.log1p(train_eda['duration'])\ntrain_eda['balance_log'] = np.log1p(train_eda['balance'])\n\n# Square root transformation\ntrain_eda['campaign_sqrt'] = np.sqrt(train_eda['campaign'])\ntrain_eda['duration_sqrt'] = np.sqrt(train_eda['duration'])\ntrain_eda['balance_sqrt'] = np.sqrt(train_eda['balance'])\n\n# Exponential transformation\n\ntrain_eda['campaign_exp'] = np.exp(train_eda['campaign'] / train_eda['campaign'].max())\ntrain_eda['duration_exp'] = np.exp(train_eda['duration'] / train_eda['duration'].max())\ntrain_eda['balance_exp'] = np.exp(train_eda['balance'] / train_eda['balance'].max())\n\n# Create plots\nplt.figure(figsize=(18, 15))\n\n# Campaign plots\nplt.subplot(3, 3, 1)\nsns.histplot(train_eda['campaign_log'], bins=10, kde=True, color='lightblue')\nplt.title(\"Log-Transformed Campaign Distribution\")\nplt.xlabel(\"Log of Campaign\")\nplt.ylabel(\"Frequency\")\n\nplt.subplot(3, 3, 2)\nsns.histplot(train_eda['campaign_sqrt'], bins=10, kde=True, color='lightcoral')\nplt.title(\"Square Root-Transformed Campaign Distribution\")\nplt.xlabel(\"Square Root of Campaign\")\nplt.ylabel(\"Frequency\")\n\nplt.subplot(3, 3, 3)\nsns.histplot(train_eda['campaign_exp'], bins=10, kde=True, color='lightgreen')\nplt.title(\"Exponential-Transformed Campaign Distribution\")\nplt.xlabel(\"Exponential of Normalized Campaign\")\nplt.ylabel(\"Frequency\")\n\n# Duration plots\nplt.subplot(3, 3, 4)\nsns.histplot(train_eda['duration_log'], bins=10, kde=True, color='lightblue')\nplt.title(\"Log-Transformed Duration Distribution\")\nplt.xlabel(\"Log of Duration\")\nplt.ylabel(\"Frequency\")\n\nplt.subplot(3, 3, 5)\nsns.histplot(train_eda['duration_sqrt'], bins=10, kde=True, color='lightcoral')\nplt.title(\"Square Root-Transformed Duration Distribution\")\nplt.xlabel(\"Square Root of Duration\")\nplt.ylabel(\"Frequency\")\n\nplt.subplot(3, 3, 6)\nsns.histplot(train_eda['duration_exp'], bins=10, kde=True, color='lightgreen')\nplt.title(\"Exponential-Transformed Duration Distribution\")\nplt.xlabel(\"Exponential of Normalized Duration\")\nplt.ylabel(\"Frequency\")\n\n# Balance plots\nplt.subplot(3, 3, 7)\nsns.histplot(train_eda['balance_log'], bins=10, kde=True, color='lightblue')\nplt.title(\"Log-Transformed Balance Distribution\")\nplt.xlabel(\"Log of Balance\")\nplt.ylabel(\"Frequency\")\n\nplt.subplot(3, 3, 8)\nsns.histplot(train_eda['balance_sqrt'], bins=10, kde=True, color='lightcoral')\nplt.title(\"Square Root-Transformed Balance Distribution\")\nplt.xlabel(\"Square Root of Balance\")\nplt.ylabel(\"Frequency\")\n\nplt.subplot(3, 3, 9)\nsns.histplot(train_eda['balance_exp'], bins=10, kde=True, color='lightgreen')\nplt.title(\"Exponential-Transformed Balance Distribution\")\nplt.xlabel(\"Exponential of Normalized Balance\")\nplt.ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T08:06:51.004013Z","iopub.execute_input":"2024-11-10T08:06:51.005170Z","iopub.status.idle":"2024-11-10T08:06:57.024353Z","shell.execute_reply.started":"2024-11-10T08:06:51.005070Z","shell.execute_reply":"2024-11-10T08:06:57.022893Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**log transformation** seems more appropriate for all the variables present above","metadata":{}},{"cell_type":"code","source":"train_eda['last contact date'] = pd.to_datetime(train_eda['last contact date'])\ntrain_eda['day'] = train_eda['last contact date'].dt.day\ntrain_eda['month'] = train_eda['last contact date'].dt.month\ntrain_eda['year'] = train_eda['last contact date'].dt.year\n\nsuggest_bins(train_eda,'year')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T00:58:38.336534Z","iopub.execute_input":"2024-11-11T00:58:38.337430Z","iopub.status.idle":"2024-11-11T00:58:38.783546Z","shell.execute_reply.started":"2024-11-11T00:58:38.337365Z","shell.execute_reply":"2024-11-11T00:58:38.781977Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{}},{"cell_type":"code","source":"\n\n#splitting the dataset into train,val and test\nfrom sklearn.model_selection import train_test_split\n\n#copy of the original data to work on\ndf_train=train_data.copy()\ndf_test=test_data.copy()\n\n\ntrain_target=df_train[\"target\"]\ndf_train=df_train.drop(columns=\"target\")\nX_train,X_val,y_train,y_val=train_test_split(df_train,train_target,test_size=0.2,stratify=train_target)\nX_train.shape,X_val.shape,y_train.shape,y_val.shape\n\nle=LabelEncoder()\ny_train=le.fit_transform(y_train)\ny_val=le.transform(y_val)\n\n##Encoding the labels (yes/no ->1/0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T16:17:41.849367Z","iopub.execute_input":"2024-11-30T16:17:41.849788Z","iopub.status.idle":"2024-11-30T16:17:41.918946Z","shell.execute_reply.started":"2024-11-30T16:17:41.849750Z","shell.execute_reply":"2024-11-30T16:17:41.917441Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dealing with missing values and encoding poutcome","metadata":{}},{"cell_type":"code","source":"#impute missing values poutcome, job, contact, education\n\ndef impute_poutcome(df):\n    df[\"poutcome\"].fillna(\"no_previous\",inplace=True)\n    return df\n \n\npipeline = Pipeline([\n   \n    ('imputer', ColumnTransformer([\n        ('si', SimpleImputer(strategy=\"most_frequent\"), [\"job\", \"contact\", \"education\"])\n    ], remainder=\"passthrough\", verbose_feature_names_out=False).set_output(transform=\"pandas\"))\n])\n\nX_train=impute_poutcome(X_train)\nX_val=impute_poutcome(X_val)\ndf_test=impute_poutcome(df_test)\nX_train=pipeline.fit_transform(X_train)\nX_val=pipeline.transform(X_val)\ndf_test=pipeline.transform(df_test)\n\nprint('No missing values in the train dataset' if X_train.isna().sum().sum() == 0 else 'There are missing values in the test dataset')\nprint('No missing values in the test dataset' if df_test.isna().sum().sum() == 0 else 'There are missing values in the test dataset')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T16:17:42.429874Z","iopub.execute_input":"2024-11-30T16:17:42.430273Z","iopub.status.idle":"2024-11-30T16:17:42.528264Z","shell.execute_reply.started":"2024-11-30T16:17:42.430240Z","shell.execute_reply":"2024-11-30T16:17:42.526838Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Removing outliers","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom sklearn.pipeline import FunctionTransformer\n\ndef remove_outliers_balanced(X_train, y_train, columns_to_check, threshold=3): #threshold for z score calculation\n    \n    y_numpy = np.array(y_train)\n    \n    initial_zeros = (y_numpy == 0).sum()  #number of 1s in the original df\n    initial_ones = (y_numpy == 1).sum()  #number of 0s in the original df\n    initial_ratio = initial_zeros / initial_ones #ratio b/w the above both, since, the aim is to remove balanced\n    \n    zeros_mask = (y_numpy == 0)\n    ones_mask = (y_numpy == 1)\n    \n    keep_mask = np.ones(len(y_numpy), dtype=bool)\n    \n    for column in columns_to_check:\n        column_values = X_train[column].values.astype(float)\n        \n        zeros_values = column_values[zeros_mask]\n        ones_values = column_values[ones_mask]\n        \n        # Remove NaN values for z-score calculation\n        zeros_values = zeros_values[~np.isnan(zeros_values)]\n        ones_values = ones_values[~np.isnan(ones_values)]\n        \n        if len(zeros_values) > 0:\n            # Calculate z-scores for zeros class\n            mean_zeros = np.mean(zeros_values)\n            std_zeros = np.std(zeros_values)\n            if std_zeros != 0:\n                z_scores_zeros = np.abs((column_values[zeros_mask] - mean_zeros) / std_zeros)\n                temp_zeros_outliers = np.zeros(len(y_numpy), dtype=bool)\n                temp_zeros_outliers[zeros_mask] = (z_scores_zeros > threshold)\n                keep_mask = keep_mask & ~temp_zeros_outliers\n        \n        if len(ones_values) > 0:\n            # Calculate z-scores for ones class\n            mean_ones = np.mean(ones_values)\n            std_ones = np.std(ones_values)\n            if std_ones != 0:\n                z_scores_ones = np.abs((column_values[ones_mask] - mean_ones) / std_ones)\n                temp_ones_outliers = np.zeros(len(y_numpy), dtype=bool)\n                temp_ones_outliers[ones_mask] = (z_scores_ones > threshold)\n                keep_mask = keep_mask & ~temp_ones_outliers\n    \n    # Count outliers for each class\n    zeros_outliers = (~keep_mask & zeros_mask).sum()\n    ones_outliers = (~keep_mask & ones_mask).sum()\n    total_outliers = zeros_outliers + ones_outliers\n    \n    if total_outliers > 0:\n        # Calculate target numbers for 90-10 split\n        target_zeros_remove = int(0.8 * total_outliers)\n        target_ones_remove = total_outliers - target_zeros_remove\n        \n        # Adjust outlier removal if needed\n        if zeros_outliers > target_zeros_remove:\n            # Randomly select which outliers to keep\n            outlier_indices = np.where(~keep_mask & zeros_mask)[0]\n            keep_indices = np.random.choice(outlier_indices, \n                                          size=zeros_outliers - target_zeros_remove,\n                                          replace=False)\n            keep_mask[keep_indices] = True\n        \n        if ones_outliers > target_ones_remove:\n            outlier_indices = np.where(~keep_mask & ones_mask)[0]\n            keep_indices = np.random.choice(outlier_indices,\n                                          size=ones_outliers - target_ones_remove,\n                                          replace=False)\n            keep_mask[keep_indices] = True\n    \n    # Apply the mask to get clean data\n    X_train_clean = X_train.iloc[keep_mask].copy()\n    y_train_clean = y_numpy[keep_mask]\n    \n    # Calculate final ratios\n    final_zeros = (y_train_clean == 0).sum()\n    final_ones = (y_train_clean == 1).sum()\n    final_ratio = final_zeros / final_ones\n    \n    print(f\"Initial class ratio (0:1): {initial_ratio:.2f}\")\n    print(f\"Final class ratio (0:1): {final_ratio:.2f}\")\n    print(f\"Removed {(~keep_mask & zeros_mask).sum()} samples from class 0\")\n    print(f\"Removed {(~keep_mask & ones_mask).sum()} samples from class 1\")\n    \n    return X_train_clean, y_train_clean\n    #\"campaign\",\ncolumns_to_check=[\"balance\",\"duration\",\"campaign\"]\n\n\nX_train,y_train=remove_outliers_balanced(X_train,y_train,columns_to_check)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T16:17:43.153993Z","iopub.execute_input":"2024-11-30T16:17:43.154402Z","iopub.status.idle":"2024-11-30T16:17:43.187672Z","shell.execute_reply.started":"2024-11-30T16:17:43.154361Z","shell.execute_reply":"2024-11-30T16:17:43.186388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#numerical\n# def encode_date(df):\n#     df['last contact date'] = pd.to_datetime(df['last contact date'])\n#     df['day'] = df['last contact date'].dt.day\n#     df['month'] = df['last contact date'].dt.month\n#     df['year'] = df['last contact date'].dt.year\n#     df['day_sin'] = np.sin(2 * np.pi * df['day']/31)\n#     df['day_cos'] = np.cos(2 * np.pi * df['day']/31)\n#     df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n#     df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n#     df.drop(columns=['last contact date','month','day'],inplace=True)\n#     return df\ndef encode_date(df):\n    df['last contact date'] = pd.to_datetime(df['last contact date'])\n    df['day'] = df['last contact date'].dt.day\n    df['month'] = df['last contact date'].dt.month\n    df['year'] = df['last contact date'].dt.year\n    return df.drop(columns=['last contact date'])\n\ndef transform_balance_and_duration(df):\n    df['balance'] = df['balance'].apply(lambda x: max(0, x)) #balance has negative values\n    df['balance'] = np.log1p(df['balance'])\n    df['duration'] = np.log1p(df['duration'])\n    df['campaign']=np.log1p(df['campaign'])\n    return df\ndef pdpr(df):\n    df[\"prepr\"]=df[\"previous\"]*df[\"pdays\"]\n    return df\n\ndef is_first_contact(df, pdays_col='pdays'):\n    \"\"\"\n    Adds a binary column 'is_first_contact' to indicate if it's the first contact (True when pdays == -1).\n\n    Parameters:\n    df (pd.DataFrame): The input DataFrame.\n    pdays_col (str): The name of the column representing 'pdays'.\n\n    Returns:\n    pd.DataFrame: The modified DataFrame with the 'is_first_contact' column added.\n    \"\"\"\n    df['is_first_contact'] = df[pdays_col] == -1\n    return df\n\ndef bin_age_kmeans(df, age_col='age', bins=None):\n    \"\"\"\n    Bins the age column into specified ranges using K-means-derived intervals.\n\n    Parameters:\n    df (pd.DataFrame): The input DataFrame.\n    age_col (str): The name of the column representing age.\n    bins (list): The edges of the bins. Example: [18.00, 30.96, 42.05, 54.91, 79.51, 95.00].\n    bin_col_name (str): The name of the new column to store the binned data.\n\n    Returns:\n    pd.DataFrame: The modified DataFrame with the new binned column added.\n    \"\"\"\n    if bins is None:\n        bins = [18.00, 30.96, 42.05, 54.91, 79.51, 95.00]\n    \n    # Create bin labels\n    bin_labels = [f'{i+1}' for i in range(len(bins) - 1)]\n    \n    # Bin the ages\n    df[age_col] = pd.cut(df[age_col], bins=bins, labels=bin_labels, include_lowest=True)\n    \n    return df\n\ndef bin_balance_kmeans(df, balance_col='balance', bins=None, bin_col_name='balance_bin'):\n    \"\"\"\n    Bins the balance column into specified ranges using K-means-derived intervals.\n\n    Parameters:\n    df (pd.DataFrame): The input DataFrame.\n    balance_col (str): The name of the column representing the balance.\n    bins (list): The edges of the bins. Example: [-8019.00, 1200.38, 24255.18, 56011.15, 86641.58, 102127.00].\n    bin_col_name (str): The name of the new column to store the binned data.\n\n    Returns:\n    pd.DataFrame: The modified DataFrame with the new binned column added.\n    \"\"\"\n    if bins is None:\n        bins = [-8019.00, 1200.38, 24255.18, 56011.15, 86641.58, 102127.00]\n    \n    # Create bin labels\n    bin_labels = [f' {i+1}' for i in range(len(bins) - 1)]\n    \n    # Bin the balances\n    df[bin_col_name] = pd.cut(df[balance_col], bins=bins, labels=bin_labels, include_lowest=True)\n    \n    return df\n\nimport pandas as pd\n\ndef bin_previous_column(df):\n    \"\"\"\n    Bins the 'previous' column of a DataFrame into specified ranges using predefined bin edges.\n\n    Parameters:\n        df (pd.DataFrame): The input DataFrame.\n\n    Returns:\n        pd.DataFrame: The DataFrame with an additional column for the binned values (as integers).\n    \"\"\"\n    bin_edges = [0.00, 0.82, 82.99, 162.46, 238.65, 275.00]\n    bin_labels = [i+1 for i in range(len(bin_edges) - 1)]  #\n\n    \n    df['previous_binned'] = pd.cut(\n        df['previous'], \n        bins=bin_edges, \n        labels=bin_labels, \n        include_lowest=True\n    ).astype(int)  # Ensure the binned column is of integer type\n    df[\"previous_binned\"]=df[\"previous_binned\"]*df[\"pdays\"]\n    return df\n    \n    return df\npipeline = Pipeline([\n    #('poutcome_replace', FunctionTransformer(replace_poutcome)),\n    ('date_encoding', FunctionTransformer(encode_date)),\n    ('age',FunctionTransformer(bin_age_kmeans)),\n    ('pdays',FunctionTransformer(is_first_contact)),\n    ('prev',FunctionTransformer(bin_previous_column)),\n    ('lo',FunctionTransformer(bin_balance_kmeans)),\n    #('prpr',FunctionTransformer(pdpr)),\n    ('balance_duration_transform', FunctionTransformer(transform_balance_and_duration)),\n])\n\nX_train = pipeline.fit_transform(X_train)\nX_val = pipeline.transform(X_val)\ndf_test = pipeline.transform(df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T16:17:43.727196Z","iopub.execute_input":"2024-11-30T16:17:43.727666Z","iopub.status.idle":"2024-11-30T16:17:43.833375Z","shell.execute_reply.started":"2024-11-30T16:17:43.727626Z","shell.execute_reply":"2024-11-30T16:17:43.832037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.drop(columns=[\"previous\"],inplace=True)\nX_val.drop(columns=[\"previous\"],inplace=True)\ndf_test.drop(columns=[\"previous\"],inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T16:17:44.011796Z","iopub.execute_input":"2024-11-30T16:17:44.012475Z","iopub.status.idle":"2024-11-30T16:17:44.029888Z","shell.execute_reply.started":"2024-11-30T16:17:44.012415Z","shell.execute_reply":"2024-11-30T16:17:44.028366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#convert boolean variables to binary\ndef convert_booleans_to_binary(X_train, X_val, df_test):\n\n    for df in [X_train, X_val, df_test]:\n        bool_cols = df.select_dtypes(include='bool').columns\n        df[bool_cols] = df[bool_cols].astype(int)\n    \n    return X_train, X_val, df_test\n\n# Apply to encoded data\n\nX_train, X_val, df_test = convert_booleans_to_binary(X_train, X_val, df_test)\nX_train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T16:17:44.694220Z","iopub.execute_input":"2024-11-30T16:17:44.695469Z","iopub.status.idle":"2024-11-30T16:17:44.729968Z","shell.execute_reply.started":"2024-11-30T16:17:44.695426Z","shell.execute_reply":"2024-11-30T16:17:44.728579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Encoding categorical \n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Define a custom transformer for target encoding\nclass TargetEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, feature, target):\n        self.feature = feature\n        self.target = target\n        self.encoding_map = {}\n    \n    def fit(self, X, y):\n        # Calculate mean of the target for each category\n        self.encoding_map = X.groupby(self.feature).apply(lambda x: y[X.index.isin(x.index)].mean())\n        return self\n    \n    def transform(self, X):\n        # Map the encoding to the feature\n        X_transformed = X[self.feature].map(self.encoding_map)\n        return X_transformed.values.reshape(-1, 1)  # Reshape for compatibility\n\ncategorical_features_ohe = [ \"contact\"] #\"\ncategorical_features_ord=[\"housing\",\"loan\",\"poutcome\",\"default\",\"marital\",\"contact\",\"job\"]\nordinal_features = [\"education\"] \ntarget_features = [\"job\", \"poutcome\"]\n\njob_target_encoder = TargetEncoder(feature='job', target=y_train)\nmarital_target_encoder=TargetEncoder(feature='marital', target=y_train)\n\ncat_ct = ColumnTransformer(\n    transformers=[\n        #(\"ohe\", OneHotEncoder(), categorical_features_ohe),\n        (\"labelencoder\",OrdinalEncoder(),categorical_features_ord),\n        (\"ord\", OrdinalEncoder(), ordinal_features),\n        #(\"job_te\", job_target_encoder, [\"job\"]),\n        #(\"marital\",marital_target_encoder,[\"marital\"]),\n        #(\"poutcome_te\", poutcome_target_encoder, [\"poutcome\"]),\n        # (\"contact_te\",contact_target_encoder,[\"contact\"]),\n        # (\"default_target_encoder\",default_target_encoder,[\"default\"]),\n        # (\"housing_target_encoder\",housing_target_encoder,[\"housing\"]),\n        # (\"loan_target_encoder\",loan_target_encoder,[\"loan\"])\n    ],\n    remainder=\"passthrough\"\n)\n\n\ncat_ct.fit(X_train, y_train)\nX_train_transformed = cat_ct.transform(X_train)\nX_val_transformed = cat_ct.transform(X_val)\ndf_test_transformed = cat_ct.transform(df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T16:17:45.163281Z","iopub.execute_input":"2024-11-30T16:17:45.163711Z","iopub.status.idle":"2024-11-30T16:17:45.380912Z","shell.execute_reply.started":"2024-11-30T16:17:45.163675Z","shell.execute_reply":"2024-11-30T16:17:45.379689Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Resampling","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.utils import resample\n\n\ndf_train = pd.DataFrame(X_train_transformed)  \ndf_train['target'] = y_train     \n\n\nmajority_class = df_train[df_train['target'] == 0]\nminority_class = df_train[df_train['target'] == 1]\n\n\nminority_upsampled = resample(minority_class,\n                              replace=True,     \n                              n_samples=len(majority_class),  \n                              random_state=42) \n\n\nupsampled = pd.concat([majority_class, minority_upsampled], ignore_index=True)\n\n\nX_train_upsampled = upsampled.drop('target', axis=1).values  \ny_train_upsampled = upsampled['target'].values ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T15:27:26.184791Z","iopub.execute_input":"2024-11-30T15:27:26.185199Z","iopub.status.idle":"2024-11-30T15:27:26.256847Z","shell.execute_reply.started":"2024-11-30T15:27:26.185162Z","shell.execute_reply":"2024-11-30T15:27:26.255603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from imblearn.combine import SMOTEENN\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\n# First let's try SMOTEENN\nsmote_enn = SMOTEENN(random_state=42,sampling_strategy=0.5)\nX_train_upsampled, y_train_upsampled = smote_enn.fit_resample(X_train_transformed, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T16:17:46.698304Z","iopub.execute_input":"2024-11-30T16:17:46.699459Z","iopub.status.idle":"2024-11-30T16:17:50.353150Z","shell.execute_reply.started":"2024-11-30T16:17:46.699414Z","shell.execute_reply":"2024-11-30T16:17:50.352266Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training and Validating","metadata":{"execution":{"iopub.status.busy":"2024-11-10T10:57:03.828031Z","iopub.execute_input":"2024-11-10T10:57:03.829421Z","iopub.status.idle":"2024-11-10T10:57:03.834278Z","shell.execute_reply.started":"2024-11-10T10:57:03.829359Z","shell.execute_reply":"2024-11-10T10:57:03.833128Z"}}},{"cell_type":"code","source":"\ndef plot_confusion_matrix(cm, title='Confusion Matrix', cmap=plt.cm.Blues):\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, cbar=False, \n                xticklabels=['Predicted 0', 'Predicted 1'], \n                yticklabels=['Actual 0', 'Actual 1'])\n    plt.title(title)\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T15:11:27.379871Z","iopub.execute_input":"2024-11-30T15:11:27.380248Z","iopub.status.idle":"2024-11-30T15:11:27.387007Z","shell.execute_reply.started":"2024-11-30T15:11:27.380213Z","shell.execute_reply":"2024-11-30T15:11:27.385768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\ndef plot_roc_curve(y_true, y_probs):\n    # Calculate the ROC curve points and AUC\n    fpr, tpr, _ = roc_curve(y_true, y_probs)\n    roc_auc = auc(fpr, tpr)\n    \n    # Plotting\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line for random classifier\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc='lower right')\n    plt.grid()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T15:03:22.731878Z","iopub.execute_input":"2024-11-30T15:03:22.732187Z","iopub.status.idle":"2024-11-30T15:03:22.745426Z","shell.execute_reply.started":"2024-11-30T15:03:22.732157Z","shell.execute_reply":"2024-11-30T15:03:22.744394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_curve, auc\n\ndef plot_pr_curve(y_true, y_probs):\n\n    precision, recall, _ = precision_recall_curve(y_true, y_probs)\n    pr_auc = auc(recall, precision)\n    \n    plt.figure(figsize=(8, 6))\n    plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.2f}', color='purple')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    plt.legend(loc='lower left')\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T15:03:22.747479Z","iopub.execute_input":"2024-11-30T15:03:22.747907Z","iopub.status.idle":"2024-11-30T15:03:22.759122Z","shell.execute_reply.started":"2024-11-30T15:03:22.747873Z","shell.execute_reply":"2024-11-30T15:03:22.758013Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n\nparam_grid_log_reg = {\n    'logisticregression__penalty': ['l2','l1'], \n    'logisticregression__C': [0.1, 1, 10], \n    'logisticregression__solver': ['lbfgs', 'liblinear','saga','newton-cg']  \n}\n\nlog_reg = LogisticRegression(random_state=42,max_iter=2000,)\npipeline = make_pipeline(StandardScaler(), log_reg)\ngrid_search_log_reg = GridSearchCV(estimator=pipeline, param_grid=param_grid_log_reg, \n                                   scoring='f1_macro', cv=2, verbose=1, n_jobs=-1)\n\ngrid_search_log_reg.fit(X_train_upsampled, y_train_upsampled)\n\nbest_log_reg_model = grid_search_log_reg.best_estimator_\n\ny_pred_log_reg = best_log_reg_model.predict(X_val_transformed)\ny_pred_log_probs=best_log_reg_model.predict_proba(X_val_transformed)\nf1_log_reg = f1_score(y_val, y_pred_log_reg, average='macro')\nprint(f\"Best Logistic Regression F1 Score: {f1_log_reg}\")\n\ncm_log_reg = confusion_matrix(y_val, y_pred_log_reg)\ndisp_log_reg = ConfusionMatrixDisplay(confusion_matrix=cm_log_reg)\ndisp_log_reg.plot(cmap='Blues')\nplt.title(\"Confusion Matrix - Logistic Regression (GridSearch)\")\nplt.show()\n\n\nplot_roc_curve(y_val,y_pred_log_probs[:,1])\nplot_pr_curve(y_val,y_pred_log_probs[:,1])\n\nprint(\"Best parameters found by GridSearchCV:\", grid_search_log_reg.best_params_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T04:58:08.222773Z","iopub.execute_input":"2024-11-23T04:58:08.223282Z"},"_kg_hide-output":false,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Insights from Logistic Regression Model**\n<div style=\"font-family:Verdana, sans-serif; color:blue;\"> \n<p>F1 Score: The logistic regression model achieved a reasonable F1 score of 0.716, balancing precision and recall, though there is room for improvement.</p>\n<p>Best Hyperparameters: Optimal hyperparameters identified by GridSearchCV: **C=10, penalty=‘l2’, and solver=‘liblinear’**, indicating a well-regularized model.</p>\n <p>Confusion Matrix: With 5337 True Negatives and 970 True Positives, the model performs better at predicting negatives, but false positives (1340) show room for improvement in handling class imbalance. </p>\n</div>","metadata":{}},{"cell_type":"code","source":"# from sklearn.svm import SVC\n# from sklearn.pipeline import make_pipeline\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n# import matplotlib.pyplot as plt\n\n\n# svm_model = SVC(C=1, kernel='rbf', gamma='scale', random_state=42,probability=True)\n\n# svm_pipeline = make_pipeline(StandardScaler(), svm_model)\n\n# svm_pipeline.fit(X_train_upsampled, y_train_upsampled)\n\n# y_pred_svm = svm_pipeline.predict(X_val_transformed)\n# y_pred_svm_probs = svm_pipeline.predict_proba(X_val_transformed)\n\n\n# f1_svm = f1_score(y_val, y_pred_svm, average='macro')\n# print(f\"Best SVM F1 Score (Validation): {f1_svm}\")\n\n# y_train_pred_svm = svm_pipeline.predict(X_train_upsampled)\n# f1_train_svm = f1_score(y_train_upsampled, y_train_pred_svm, average='macro')\n# print(f\"Training F1 Score: {f1_train_svm}\")\n\n# # Confusion Matrix\n# cm_svm = confusion_matrix(y_val, y_pred_svm)\n# disp_svm = ConfusionMatrixDisplay(confusion_matrix=cm_svm)\n# disp_svm.plot(cmap='Blues')\n# plt.title(\"Confusion Matrix - SVM\")\n# plt.show()\n\n# # ROC Curve\n# plot_roc_curve(y_val, y_pred_svm_probs[:, 1])\n\n# # PR Curve\n# plot_pr_curve(y_val, y_pred_svm_probs[:, 1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T05:54:47.683377Z","iopub.status.idle":"2024-11-11T05:54:47.683957Z","shell.execute_reply.started":"2024-11-11T05:54:47.683681Z","shell.execute_reply":"2024-11-11T05:54:47.683712Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score, confusion_matrix\nimport numpy as np\n\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [4, 6, 8, 10], \n    'max_leaf_nodes': [5, 10, 15, 20], \n    'min_samples_split': [0.1, 0.2, 0.3],\n    'class_weight': ['balanced', None]\n}\n\ndecision_tree = DecisionTreeClassifier(random_state=42)\n\ngrid_search = GridSearchCV(\n    estimator=decision_tree,\n    param_grid=param_grid,\n    scoring='f1_macro',  \n    cv=5, \n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(X_train_upsampled, y_train_upsampled)\n\nbest_dt_model = grid_search.best_estimator_\n\ny_val_dt = best_dt_model.predict(X_val_transformed)\n\n# Evaluation metrics\ndt_train_f1 = f1_score(y_train_upsampled, best_dt_model.predict(X_train_upsampled), average='macro')\ndt_val_f1 = f1_score(y_val, y_val_dt, average='macro')\ndt_confusion = confusion_matrix(y_val, y_val_dt)\n\nprint(\"\\nDecision Tree Best Parameters:\", grid_search.best_params_)\nprint(\"\\nDecision Tree Train F1 Score:\", dt_train_f1)\nprint(\"Decision Tree Validation F1 Score:\", dt_val_f1)\nprint(\"\\nDecision Tree Confusion Matrix:\")\nprint(dt_confusion)\n\n# Plot confusion matrix\ny_val_dt_probs=best_dt_model.predict_proba(X_val_transformed)\nplot_confusion_matrix(dt_confusion, title='Decision Tree Confusion Matrix (Best Model)')\nplot_roc_curve(y_val,y_val_dt_probs[:,1])\nplot_pr_curve(y_val,y_val_dt_probs[:,1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T02:51:32.650631Z","iopub.execute_input":"2024-11-11T02:51:32.651065Z","iopub.status.idle":"2024-11-11T02:52:30.256925Z","shell.execute_reply.started":"2024-11-11T02:51:32.651017Z","shell.execute_reply":"2024-11-11T02:52:30.255593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, f1_score\nfrom sklearn.model_selection import cross_val_score\n\n# Initialize Random Forest model\nrf_model = RandomForestClassifier(random_state=42)\n\n# Random Forest Cross-Validation\nprint(\"Random Forest Cross-Validation\")\nrf_cv_scores = cross_val_score(rf_model, X_train_upsampled, y_train_upsampled, scoring='f1_macro', cv=5)\nprint(\"Random Forest Cross-Validation F1 Scores:\", rf_cv_scores)\nprint(\"Random Forest Average CV F1 Score:\", np.mean(rf_cv_scores))\n\n\nrf_model.fit(X_train_upsampled, y_train_upsampled)\n\ny_val_pred_rf = rf_model.predict(X_val_transformed)\ny_val_rf_probs=rf_model.predict_proba(X_val_transformed)\n\n\nrf_confusion = confusion_matrix(y_val, y_val_pred_rf)\nprint(\"\\nRandom Forest Confusion Matrix:\")\nprint(rf_confusion)\nprint(\"Random Forest Train F1 Score:\", f1_score(y_train_upsampled, rf_model.predict(X_train_upsampled), average='macro'))\nprint(\"Random Forest Validation F1 Score:\", f1_score(y_val, y_val_pred_rf, average='macro'))\n\n\nplot_confusion_matrix(rf_confusion, title='Random Forest Confusion Matrix')\nplot_roc_curve(y_val,y_val_rf_probs[:,1])\nplot_pr_curve(y_val,y_val_rf_probs[:,1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T02:52:30.258625Z","iopub.execute_input":"2024-11-11T02:52:30.259101Z","iopub.status.idle":"2024-11-11T02:53:10.432083Z","shell.execute_reply.started":"2024-11-11T02:52:30.259045Z","shell.execute_reply":"2024-11-11T02:53:10.430780Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- **Random Forest model shows strong performance**, with an average cross-validation F1 score of 0.9676, indicating good generalization across multiple folds.\n- **Confusion matrix shows high accuracy**, with a relatively low number of false positives (616) and false negatives (406), suggesting good classification of both classes.\n- **Training F1 score of 1.0** indicates that the model perfectly fits the training data, potentially suggesting overfitting\n- **High F1 score consistency** across the cross-validation folds shows that the model is not sensitive to different subsets of the data and can generalize well.\n- **Random Forest's performance** is likely attributed to its ability to model complex relationships through ensemble learning and decision trees, reducing variance and avoiding overfitting.","metadata":{"execution":{"iopub.status.busy":"2024-11-10T12:21:56.311159Z","iopub.execute_input":"2024-11-10T12:21:56.312018Z","iopub.status.idle":"2024-11-10T12:21:56.322134Z","shell.execute_reply.started":"2024-11-10T12:21:56.311968Z","shell.execute_reply":"2024-11-10T12:21:56.320506Z"}}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, f1_score\nfrom sklearn.model_selection import cross_val_score\n\n\ndef f1_eval(y_true, y_pred):\n    f1 = f1_score(y_true, y_pred)\n    return 'f1', f1\n\n\nxgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n\n\nprint(\"\\nXGBoost Cross-Validation\")\nxgb_cv_scores = cross_val_score(xgb_model, X_train_upsampled, y_train_upsampled, scoring='f1_macro', cv=5)\nprint(\"XGBoost Cross-Validation F1 Scores:\", xgb_cv_scores)\nprint(\"XGBoost Average CV F1 Score:\", np.mean(xgb_cv_scores))\n\n\nxgb_model.fit(X_train_upsampled, y_train_upsampled)\ny_val_pred_xgb = xgb_model.predict(X_val_transformed)\ny_val_xgb_probs=xgb_model.predict_proba(X_val_transformed)\n\n\nxgb_confusion = confusion_matrix(y_val, y_val_pred_xgb)\nprint(\"\\nXGBoost Confusion Matrix:\")\nprint(xgb_confusion)\nprint(\"XGBoost Train F1 Score:\", f1_score(y_train_upsampled, xgb_model.predict(X_train_upsampled), average='macro'))\nprint(\"XGBoost Validation F1 Score:\", f1_score(y_val, y_val_pred_xgb, average='macro'))\n\n\nplot_confusion_matrix(xgb_confusion, title='XGBoost Confusion Matrix')\nplot_roc_curve(y_val,y_val_xgb_probs[:,1])\nplot_pr_curve(y_val,y_val_xgb_probs[:,1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T16:17:51.986620Z","iopub.execute_input":"2024-11-30T16:17:51.987154Z","iopub.status.idle":"2024-11-30T16:17:54.906432Z","shell.execute_reply.started":"2024-11-30T16:17:51.987113Z","shell.execute_reply":"2024-11-30T16:17:54.905316Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- **XGBoost model shows strong cross-validation performance**, with an average F1 score of 0.9222, demonstrating consistent results across different data splits.\n- **Confusion matrix indicates reasonable classification**, with fewer false negatives (201) and a significant number of false positives (996), suggesting a trade-off between precision and recall.\n- **Training F1 score of 0.9459** reflects that the model is effectively learning the underlying patterns in the training data, though it could indicate some overfitting due to the high training performance relative to validation.\n- **Validation F1 score of 0.7609** is lower than the training F1 score, pointing to potential overfitting or the model's difficulty generalizing to the validation set.\n- **XGBoost's performance** is likely driven by its ability to combine the predictions of many trees, allowing it to model complex patterns while controlling for overfitting through regularization and boosting.","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:20px\"> 📌 XGBoost HPT\n<href>https://datascience.stackexchange.com/questions/74571/my-xgboost-model-accuracy-decreases-after-grid-search-with</href> </div>\n<div><a>https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/</a></div>\n\n<div><a>https://datascience.stackexchange.com/questions/82028/is-it-possible-to-get-worse-model-after-optimization</a></div>\n<div><a>https://www.kaggle.com/discussions/general/197200</a></div>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV,StratifiedKFold\nimport xgboost as xgb\nfrom scipy.stats import uniform, randint\nimport numpy as np\n\n\n# Define the parameter space\nparam_distributions = {\n    \n    'max_depth': randint(4, 7),  # Shallow trees to combat overfitting\n    'min_child_weight': randint(3, 6),  # Higher values prevent overfitting\n    'learning_rate': uniform(0.01, 0.1),  # Smaller learning rate range\n    \n    'reg_alpha': uniform(0.1, 1.0),  # L1 regularization\n    'reg_lambda': uniform(1.0, 5.0),  # L2 regularization\n    'gamma': uniform(0.1, 0.5),  # Conservative split threshold\n    \n    'subsample': uniform(0.7, 0.3),  # Sample 70-100% of data\n    'colsample_bytree': uniform(0.7, 0.3),  # Sample 70-100% of features\n    \n    'n_estimators': randint(200, 500)\n}\n\n# Create XGBoost classifier\nxgb_model = xgb.XGBClassifier(\n    objective='binary:logistic', \n    random_state=42,\n    tree_method='hist'\n)\n\n#stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nrandom_search = RandomizedSearchCV(\n    estimator=xgb_model,\n    param_distributions=param_distributions,\n    n_iter=50,  \n    scoring='f1',  \n    cv=5, \n    verbose=0,\n    random_state=42,\n    n_jobs=-1 \n   \n)\neval_set = [(X_train_upsampled, y_train_upsampled), (X_val_transformed, y_val)]\nrandom_search.fit(X_train_upsampled, y_train_upsampled) #,eval_set=eval_set,verbose=False,eval_metric=[\"auc\",\"logloss\"]\nbest_model = random_search.best_estimator_\ny_val_pred_gxgb=best_model.predict(X_val_transformed)\ny_val_proba_gxbg=best_model.predict_proba(X_val_transformed)\nprint(\"XGBoost Validation F1 Score:\", f1_score(y_val,y_val_pred_gxgb,average='macro'))\nprint(\"Best parameters:\", random_search.best_params_)\nprint(\"Best cross-validation score:\", random_search.best_score_)\n\nxgb_grid_confusion = confusion_matrix(y_val, y_val_pred_gxgb)\nplot_confusion_matrix(xgb_grid_confusion, title='XGBoost Confusion Matrix')\n\nprint(\"Best model: XGBoost Validation F1 Score:\", f1_score(y_train, best_model.predict(X_train_transformed), average='macro'))\nprint(\"Best model: XGBoost Validation F1 Score:\", f1_score(y_val, best_model.predict(X_val_transformed), average='macro'))\n\nplot_roc_curve(y_val,y_val_proba_gxbg[:,1])\nplot_pr_curve(y_val,y_val_proba_gxbg[:,1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T16:02:41.481431Z","iopub.execute_input":"2024-11-30T16:02:41.481855Z","iopub.status.idle":"2024-11-30T16:04:41.937867Z","shell.execute_reply.started":"2024-11-30T16:02:41.481822Z","shell.execute_reply":"2024-11-30T16:04:41.936419Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- **XGBoost with RandomizedSearchCV** delivers an F1 validation score of **0.7672**, showing a strong balance between precision and recall.\n- **Parameter tuning** using randomized search resulted in finding optimal values for parameters like `max_depth`, `learning_rate`, `reg_alpha`, and `subsample` which help to control overfitting and improve model performance.\n- **Cross-validation with 5-fold** helps assess the model’s robustness, ensuring it performs well across different subsets of the data.\n- **Strong regularization** focus (with `reg_alpha` and `reg_lambda`) prevents overfitting, balancing the bias-variance tradeoff effectively.\n- **AUC and log-loss metrics** used during evaluation provide additional insights into model performance beyond the F1 score, focusing on overall model calibration and uncertainty.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint('XGB',classification_report(y_val,y_val_pred_xgb))\nprint('BEST XGB Randomized search',classification_report(y_val,best_model.predict(X_val_transformed)))\nprint('RandomForest',classification_report(y_val,y_val_pred_rf\n                           ))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T06:02:15.953093Z","iopub.execute_input":"2024-11-11T06:02:15.953667Z","iopub.status.idle":"2024-11-11T06:02:16.079626Z","shell.execute_reply.started":"2024-11-11T06:02:15.953625Z","shell.execute_reply":"2024-11-11T06:02:16.078105Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"loss curve from training a machine learning model, displaying both training loss (blue line) and validation loss (orange line) over approximately 450 epochs.\n\nThere are several key observations that indicate this model is experiencing overfitting:\n\n1. Diverging Lines: The training loss (blue) continues to decrease steadily throughout training, while the validation loss (orange) initially decreases but then begins to increase and plateau after around epoch 50.\n\n2. Performance Gap: There's a growing gap between training and validation loss - the training loss keeps improving (decreasing) while the validation loss gets worse (increases), which is a classic sign of overfitting.\n\n3. Early Minimum: The validation loss reaches its minimum point relatively early in training (around epoch 50) and then starts to increase, suggesting this would have been a good point for early stopping.\n\nTo address this overfitting, several techniques could be considered:\n- Implementing early stopping around epoch 50\n- Adding regularization (L1/L2)\n- Using dropout\n- Reducing model complexity\n- Increasing the training data\n- Using data augmentation\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV,StratifiedKFold\nimport xgboost as xgb\nfrom scipy.stats import uniform, randint\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n\n\n# Define the parameter space\nparam_distributions = {\n    \n    'max_depth': randint(2, 10),  # Shallow trees to combat overfitting\n    'min_child_weight': randint(1, 8),  # Higher values prevent overfitting\n    'learning_rate': uniform(0.001, 0.1),  # Smaller learning rate range\n    \n    'reg_alpha': uniform(0.01, 1.0),  # L1 regularization\n    'reg_lambda': uniform(1.0, 5.0),  # L2 regularization\n    'gamma': uniform(0.1, 0.5),  # Conservative split threshold\n    \n    'subsample': uniform(0.7, 0.3),  # Sample 70-100% of data\n    'colsample_bytree': uniform(0.7, 0.3),  # Sample 70-100% of features\n    \n    'n_estimators': randint(100, 500),\n    'scale_pos_weight':[1,2,3,4,5]\n}\n\n# Create XGBoost classifier\nxgb_model = xgb.XGBClassifier(\n    objective='binary:logistic', \n    random_state=42,\n    tree_method='hist'\n)\n\nstratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nrandom_search = RandomizedSearchCV(\n    estimator=xgb_model,\n    param_distributions=param_distributions,\n    n_iter=50,  \n    scoring='f1',  \n    cv=stratified_kfold, \n    verbose=0,\n    random_state=42,\n    n_jobs=-1\n   \n)\neval_set = [(X_train_transformed, y_train), (X_val_transformed, y_val)]\nrandom_search.fit(X_train_transformed, y_train) #,eval_set=eval_set,verbose=False,eval_metric=[\"auc\",\"logloss\"]\nbest_model = random_search.best_estimator_\ny_val_pred_gxgb=best_model.predict(X_val_transformed)\ny_val_proba_gxbg=best_model.predict_proba(X_val_transformed)\nprint(\"XGBoost Validation F1 Score:\", f1_score(y_val,y_val_pred_gxgb,average='macro'))\nprint(\"Best parameters:\", random_search.best_params_)\nprint(\"Best cross-validation score:\", random_search.best_score_)\n\nxgb_grid_confusion = confusion_matrix(y_val, y_val_pred_gxgb)\nplot_confusion_matrix(xgb_grid_confusion, title='XGBoost Confusion Matrix')\n\nprint(\"Best model: XGBoost Validation F1 Score:\", f1_score(y_train, best_model.predict(X_train_transformed), average='macro'))\nprint(\"Best model: XGBoost Validation F1 Score:\", f1_score(y_val, best_model.predict(X_val_transformed), average='macro'))\n\nplot_roc_curve(y_val,y_val_proba_gxbg[:,1])\nplot_pr_curve(y_val,y_val_proba_gxbg[:,1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T16:18:25.236008Z","iopub.execute_input":"2024-11-30T16:18:25.236550Z","iopub.status.idle":"2024-11-30T16:20:57.978872Z","shell.execute_reply.started":"2024-11-30T16:18:25.236473Z","shell.execute_reply":"2024-11-30T16:20:57.977580Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Comparing classifiers","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\nimport seaborn as sns\n\ndef plot_multiple_roc_curves(y_true, *y_probs, labels):\n    # Apply a Seaborn theme\n    sns.set(style=\"whitegrid\")\n    \n    # Initialize figure for ROC curves\n    plt.figure(figsize=(10, 8))\n\n    # Iterate over each set of predicted probabilities\n    for y_pred_prob, label in zip(y_probs, labels):\n        # Calculate ROC curve and AUC\n        fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n        auc_score = auc(fpr, tpr)\n        \n        # Plot ROC curve\n        plt.plot(fpr, tpr, label=f\"{label} (AUC = {auc_score:.2f})\")\n\n    # Plot chance line\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"ROC Curves Comparison\")\n    plt.legend(loc=\"lower right\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T03:10:20.732611Z","iopub.execute_input":"2024-11-11T03:10:20.733085Z","iopub.status.idle":"2024-11-11T03:10:20.743009Z","shell.execute_reply.started":"2024-11-11T03:10:20.733042Z","shell.execute_reply":"2024-11-11T03:10:20.741431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example usage\nplot_multiple_roc_curves(\n    y_val,\n    y_pred_log_probs[:, 1],\n    y_val_dt_probs[:, 1],\n    y_val_rf_probs[:, 1],\n    y_val_xgb_probs[:, 1],\n    y_val_proba_gxbg[:, 1],\n    labels=[\"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"XGBoost\", \" XGBoost with Randomized Search\"]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T03:14:43.391468Z","iopub.execute_input":"2024-11-11T03:14:43.391972Z","iopub.status.idle":"2024-11-11T03:14:43.968154Z","shell.execute_reply.started":"2024-11-11T03:14:43.391926Z","shell.execute_reply":"2024-11-11T03:14:43.966859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n\ndef calculate_metrics_df(y_true, *y_preds, labels):\n    \n    metrics_list = []\n\n\n    for y_pred, label in zip(y_preds, labels):\n        # Compute metrics\n        precision = precision_score(y_true, y_pred)\n        recall = recall_score(y_true, y_pred)\n        accuracy = accuracy_score(y_true, y_pred)\n        f1 = f1_score(y_true, y_pred,average=\"macro\")\n        auc_score = roc_auc_score(y_true, y_pred)\n\n       \n        metrics_list.append({\n            \"Model\": label,\n            \"Precision\": precision,\n            \"Recall\": recall,\n            \"Accuracy\": accuracy,\n            \"F1 Score\": f1,\n            \"AUC Score\": auc_score\n        })\n\n   \n    metrics_df = pd.DataFrame(metrics_list)\n    return metrics_df\n\nmetrics_df = calculate_metrics_df(\n    y_val,\n    y_pred_log_reg,\n    y_val_dt,\n    y_val_pred_rf,\n    y_val_pred_xgb,\n    y_val_pred_gxgb,\n    labels=[\"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"XGBoost\", \"XGBoost with Randomized Search\"]\n)\n\nprint(metrics_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T03:24:01.556262Z","iopub.execute_input":"2024-11-11T03:24:01.556743Z","iopub.status.idle":"2024-11-11T03:24:01.662054Z","shell.execute_reply.started":"2024-11-11T03:24:01.556703Z","shell.execute_reply":"2024-11-11T03:24:01.660818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions=best_model.predict(df_test_transformed)\n\n\npredictions_mapped = ['yes' if pred == 1 else 'no' for pred in predictions]\n\n# Create submission DataFrame\nsubmission_df = pd.DataFrame({\n    'id': range(len(predictions)),  \n    'target': predictions_mapped             \n})\n\n\nsubmission_df.to_csv('submission.csv', index=False)\n(predictions==0).sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T15:30:53.226099Z","iopub.execute_input":"2024-11-30T15:30:53.226574Z","iopub.status.idle":"2024-11-30T15:30:53.280519Z","shell.execute_reply.started":"2024-11-30T15:30:53.226535Z","shell.execute_reply":"2024-11-30T15:30:53.279486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}